# Correlation

## Overview

- **Correlation** measures the relationship between two or more variables.
- Correlation coefficients range from **−1.00 to +1.00**:
  - `+1.00`: perfect positive correlation  
  - `−1.00`: perfect negative correlation  
  - `0.00`: no correlation
- The most commonly used correlation coefficient is **Pearson's r**, also known as the **product–moment correlation**.
- Pearson's r measures the **linear** relationship between two continuous variables.
- Scatter plots are useful to visualize the relationship between variables.

## Characteristics

- Values of r are unit-free (e.g., unlike standard deviation which shares the units of the data).
- The correlation coefficient must lie between −1 and +1:
  \[
  -1 \leq r \leq 1
  \]

- Other correlation metrics include:
  - **Spearman's rank correlation** (ρ)
  - **Kendall’s Tau**

These are particularly useful for **ordinal data** or **non-linear** relationships.

## Interpretation

- A **strong positive linear relationship**: when one variable increases, so does the other.
- A **strong negative linear relationship**: when one variable increases, the other decreases.
- The sample-based estimate is denoted by **r**, while the population parameter is denoted by **ρ** ("rho").

## Influence of Outliers

- **Outliers** can significantly influence the value of r, often misleading the interpretation.
- Correlation and **simple linear regression** are closely related:
  - Correlation measures linear association without implying direction or causality.
  - Regression models how a change in one (independent) variable predicts another (dependent) variable.

## Correlation vs. Causation

- A significant correlation does **not** imply a **cause-effect** relationship.
- For example:
  > Ice cream sales and flu rates might be negatively correlated—but that doesn't mean ice cream prevents the flu.
  > The real explanation could be a **latent variable**, such as **weather**:
  > - Cold weather = more flu and fewer ice cream sales  
  > - Warm weather = the opposite  

## Formula

The **Pearson correlation coefficient** is calculated as:

\[
r_{XY} = \frac{S_{XY}}{\sqrt{S_{XX} \cdot S_{YY}}}
\]

Where:
- \( S_{XY} \): covariance between X and Y  
- \( S_{XX} \), \( S_{YY} \): variance of X and Y respectively

## Summary Points

- Correlation quantifies **linear association**, not causality.
- r = +1: perfect positive linear relationship  
- r = −1: perfect negative linear relationship  
- r = 0: no linear association

> Different types of correlation coefficients exist depending on the scale and distribution of your data.

---


%========================================================================%

\subsection*{Correlation}
\begin{itemize}
	\item Recall that correlation describes the strength of a relationship between two numeric variables, and that the \textbf{\textit{Pearson product-moment correlation coefficient}} is a measure of the strength of the linear relationship between two variables.
	
	\item It is referred to as \textbf{Pearson's correlation} or simply as the correlation coefficient. If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.
	
	\item The symbol for Pearson's correlation is ``$\rho$" when it is measured in the population and \texttt{\textbf{r}} when it is measured for a sample.
	
	\item As we will be dealing almost exclusively with samples, we will use \texttt{\textbf{r}} to to represent Pearson's correlation unless otherwise noted.
	
	\item Pearson's r can range from -1 to 1. An estimate of -1 indicates a perfect negative linear relationship between variables, an \texttt{\textbf{r}} of 0 indicates no linear relationship between variables, and an \texttt{\textbf{r}} of 1 indicates a perfect positive relationship between variables.
	
	\item Importantly it is assumed that the relationship in question is supposed to be linear. Some variables will in fact have a non-linear relationship (more on that later)
\end{itemize}

\section{Pearson's Product Moment Correlation Coefficient}
\begin{itemize}
\item Pearson's product moment correlation coefficient, usually denoted by r, is one example of a correlation coefficient. It is a measure of the linear association between two variables that have been measured on interval or ratio scales, such as the relationship between height in inches and weight in pounds. 
\item 
However, it can be misleadingly small when there is a relationship between the variables but it is a non-linear one.
\item The Pearson correlation coefficient is only appropriate for
	describing the relationship between two quantitative variables
	which have a linear or near linear relationship

\item
There are procedures, based on r, for making inferences about the population correlation coefficient. 
\item However, these make the implicit assumption that the two variables are jointly normally distributed. 
When this assumption is not justified, a non-parametric measure such as the Spearman Rank Correlation Coefficient might be more appropriate.
\end{itemize}

%---------------------------------------------------------------------%
\begin{framed}
\subsection{Pearson's Correlation Coefficient.}

The Pearson correlation coefficient is a way of measuring the
strength of the relationship between two quantitative variables.

\begin{itemize}
	\item The population correlation coefficient between two variables X and
	Y is denoted by $\rho_{X,Y}$ .
	\item Used as an estimate for true correlation $\rho$.
	\item The population correlation coefficient between two variables X and
	Y is denoted by $\rho_{X,Y}$ .
	\item Used as an estimate for true correlation $\rho$.
	\item Pearson's Coefficient is denoted $r$.
	\item The Pearson Coefficient is defined to be between -1 and 1.
	\item The Pearson correlation coefficient is only appropriate for
	describing the relationship between two quantitative variables
	which have a linear or near linear relationship

	\item The Pearson Coefficient is defined to be between -1 and 1.

\end{itemize}

The Pearson Coefficient is computed using the following formula.
\[ r = \frac{S_{xy}}{(S_x)(S_y)} \]

\end{framed}
\subsection{Other Correlation Coefficients}
Pearson's Correlation Coefficient is one approach to estimating the strength of relation between two variables.
Other approaches are as follows:
\begin{itemize}
	\item Spearman's Rank Correlation
	\item Kendall Tau Correlation
\end{itemize}
These are not part of the course.



%---------------------------------------------------------------------%

%---------------------------------------------------------------------%

\subsection{Properties of the Correlation Coefficient}
\begin{enumerate}
	\item $-1 \leq r \leq +1$
	\item r = +1 or -1 represents a perfect linear correlation or a perfect linear
	relationship between the variables.
	\item r = 0 indicates little or no relationship i.e. as X increases there is no
	definite tendency for the value of Y to increase or decrease in a straight line.
	\item r close to +1 indicates a large positive correlation i.e. Y tends to increase
	as X increases.
	\item r close to -1 indicates a large negative correlation i.e. Y tends to decrease
	as X increases.
	\item The more r differs from 0, the stronger the linear relationship between the
	two variables.
	\item The sign of r indicates the direction of the relationship.
\end{enumerate}
%---------------------------------------------------------------------%


\section{Correlation and Regression}
Correlation
The Pearson's Product Moment Correlation Coefficient tells us how well two sets of continuous data correlate to each other. The value can fall between 0.00 (no correlation) and 1.00 (perfect correlation). A p value tells us if the Pearson's is significant or not. Generally p values under 0.05 are considered significant.

\subsection{Example 1}
%============================================================%

The height of a boy was observed at 7 different ages.
Comment on the relationship between height and age over this
period of time and calculate the Pearson correlation coefficient for
this data.
\begin{figure}
	% Requires \usepackage{graphicx}
	\includegraphics[scale=0.6]{images/11Bdata.jpg}\\
	
\end{figure}
\begin{itemize}
	\item X (the predictor variable) is defined to be age a
	\item Y is defined
	to be height (the dependent variable).
\end{itemize}


%Age  & 6 & 7  & 8 & 9 & 10 & 11 & 12 \\ 
%Height (cm)& 108 115& 120 &126& 132& 139 & 145\\

In order to investigate the nature of the relationship, we draw a
scatter plot.
X (the independent variable) is defined to be age and Y is defined
to be height (the dependent variable).


% http://www.statstutor.ac.uk/resources/uploaded/spearmans.pdf


\section{Correlation}

This requires a simple calculation based in values given and the relevant formula.

The formula for the Correlation estimate is as follows.

The calculated value should be between -1 and 1.

The following conclusions are drawn , depending on the Correlation estimate value:
\begin{itemize}
	\item Greater than 0.9 		Very strong positive linear relationship 
	\item Between 0.7 and 0.9		Strong positive linear relationship 
	\item Between 0.2 and 0.7	 	Weak positive linear relationship
	\item Between -0.2 and 0.2		No relationship
	\item Between -0.7 and -0.2		Weak negative linear relationship
	\item Between -0.9 and -0.7		Strong negative linear relationship
	\item Less than -0.9			Very strong negative linear relationship
\end{itemize}
Your answer should concur with your interpretation of the scatterplot.


Part 2 Correlation
This requires a simple calculation based in values given and the relevant formula.






Strength of a linear relationship between $X$ and $Y$

\begin{framed}
	\begin{verbatim}
	M=1000
	CorrData=numeric(M)
	for (i in 1:M)
	{
	CorrData[i] = cor(rnorm(10),rnorm(10))
	}
	\end{verbatim}
\end{framed}




%---------------------------------------------------------------------%

{Correlation Coefficient : Summations}
\begin{centering}
\begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{images/11Bpearson.jpg}\\
  \caption{Summations}\label{11bpear}
\end{figure}
\end{centering}
%---------------------------------------------------------------------%






\begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{images/11BPlot1.jpg}\\

\end{figure}
$r_{xy} = 0.87$ Strong Positive Linear Relationship




\begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{images/11BPlot2.jpg}\\

\end{figure}

$r_{xy} = -0.258$ (Almost) No Relationship



\begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{images/11BPlot3.jpg}\\

\end{figure}

$r_{xy} = -0.954$ Strong, though clearly non-linear




\begin{figure}
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{images/11BPlot4.jpg}\\

\end{figure}
$r_{xy} =  -0.051$ (although there is a very strong
relationship)


%{Outliers}
%Outliers can greatly influence the computed value of an estimate.
%Correlation is closely related to Simple linear regression models, in that both are concerned with the linear relationship between variables. However Linear Regression has a different emphasis.
%Simple Linear Regression describes one independent variable (IV) and the response of the dependent variable (DV).
%




{Pearson's Product Moment Correlation Coefficient}
\begin{itemize}
\item Pearson's product moment correlation coefficient, usually denoted by r, is just one example of a correlation coefficient.
\smallskip
\item However, these make the implicit assumption that the two variables are jointly normally distributed. \smallskip
\item 
When this assumption is not justified, a non-parametric measure such as the Spearman Rank Correlation Coefficient might be more appropriate.
\end{itemize}




%---------------------------------------------------------------------%
%---------------------------------------------------------------------%

\subsection{Properties of the Correlation Coefficient}
Example: We are given data for 6 graduates. Below is their
final QCA and their corresponding starting salary after
graduation.
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
 Subject & 1 &2 &3 &4 &5 &6\\
 Final QCA &2.8& 3.4& 3.2& 3.8& 3.2& 3.4\\
 Starting Salary &20 000 &24 500& 23 000& 25 000 &20 000 &22 500\\
  \hline
\end{tabular}


\begin{itemize}
\item Calculate the sample correlation coefficient.
\end{itemize}



%---------------------------------------------------------------------%

{Regression}
A statistical measure that attempts to determine the strength of the relationship between one dependent variable
(usually denoted by Y) and a series of other changing variables (known as independent variables).

Regression takes a group of random variables, thought to be predicting Y, and tries to find a mathematical relationship between them. This relationship is typically in the form of a straight line (linear regression) that best approximates all the individual data points.




%---------------------------------------------------------------------%
\section{Pearson's Correlation Coefficient}
\[ r_{XY} = \frac{Sxy}{\sqrt{SxSy}} \]

--------------------------------------------------------------------------------------
### Correlation

This requires a simple calculation based in values given and the relevant formula.

The formula for the Correlation estimate is as follows.

Sure thing, Kevin! Here’s your content neatly reformatted in Markdown for clarity, structure, and visual polish:

---

## Spearman’s Correlation Coefficient

**Spearman’s rank correlation coefficient** (\( r_s \)) is a statistical measure of the strength and direction of a *monotonic* relationship between two variables.

### Key Properties

- \( r_s \) ranges between −1 and +1:
  \[
  -1 \leq r_s \leq 1
  \]
- Interpretation is similar to **Pearson's r**: the closer \( |r_s| \) is to 1, the stronger the monotonic relationship.

### Descriptive Strength (based on absolute value of \( r_s \)):

| Range       | Description     |
|-------------|-----------------|
| 0.00–0.19   | Very weak       |
| 0.20–0.39   | Weak            |
| 0.40–0.59   | Moderate        |
| 0.60–0.79   | Strong          |
| 0.80–1.00   | Very strong     |

### Assumptions for Using Spearman's \( r_s \):

- Data must be **ordinal**, **interval**, or **ratio** level.
- Variables must be **monotonically related** (i.e. consistently increasing or decreasing).

> Note: Unlike Pearson’s correlation, Spearman's does **not** assume normality — it's a **nonparametric** statistic.

---

## May 2013: Question 6b — Correlation and Regression

**Task**: Calculate the correlation coefficient and interpret its value.

### Sample Data

| Residence | X      | Y      |
|-----------|--------|--------|
|           |        |        |
| *(data to be completed)* |        |        |

### R Implementation

To calculate the correlation coefficient in R:

```r
cor(X, Y)  # Pearson by default
```

For Spearman’s rank correlation, explicitly specify the method:

```r
cor(X, Y, method = "spearman")
```

---

## Interpreting the Correlation Coefficient

- The result is a value between −1 and 1:
  \[
  -1 \leq r \leq 1
  \]
- The strength and direction of the association can be assessed using descriptive thresholds.

---

### Example R Output

```r
> X
[1] 104.40 104.14 104.84 99.34 104.13
[6] 100.93 103.85 97.16 96.18 101.42

> Y
[1] 98.39 106.05 111.18 97.65 104.02
[6] 100.18 106.20 101.87 92.49 101.41

> cor(X, Y)
[1] 0.7171676
```

This indicates a **strong positive linear relationship** between X and Y.

---



The Pearson correlation coefficient is computed using the
following formula


\begin{itemize}
	\item $\sum x$ \item $\sum y$ \item $\sum xy$ \item $\sum x^2$
	\item $\sum y^2$
\end{itemize}

\begin{tabular}{|ccc|ccc|ccc|ccc|ccc|}
	\hline
	& X & & & Y & & &  $X^2$ & & &  $Y^2$ & & &  XY & \\
	& 1.0 & & & 10.6 & & &  1.00 & & &  36 & & &  90 & \\ \hline
	& 1.2 & & & 12.5 & & &  1.44 & & &  36 & & &  90 & \\ \hline
	& 1.6 & & & 14.7 & & &  2.56 & & &  36 & & &  90 & \\ \hline
	& 1.7 & & & 16.7 & & &  225 & & &  36 & & &  90 & \\ \hline
	& 1.8 & & & 18.7 & & &  225 & & &  36 & & &  90 & \\ \hline
	& 2.1 & & & 22.1 & & &  4.41 & & &  36 & & &  90 & \\ \hline
	
	
\end{tabular}

\[
r = { \; n \sum xy - \sum x \sum y   \; \over \left[\;\sqrt{n \sum (x^2) - (\sum x)^2} \;\right] \times  \left[ \;\sqrt{n \sum (y^2) - (\sum y)^2}\; \right]}
\]


----------------------------------------------------------------------------------------------------

## Spearman and Kendall Correlation Coefficients

- **Non-parametric statistics** do not require strong assumptions (e.g., normality).
- **Spearman's rank-order correlation** and **Kendall's Tau** are **nonparametric alternatives** to the Pearson correlation coefficient.
- Both measure the strength and direction of association between two **ranked (ordinal)** variables.
- These coefficients are interpreted in the **same way** as Pearson’s: values close to +1 or −1 indicate strong relationships; values near 0 indicate weak or no relationship.

### R Implementation

```r
# Spearman Correlation Coefficient
cor(X, Y, method = "spearman")  # Output: 0.6242424

# Kendall Correlation Coefficient
cor(X, Y, method = "kendall")   # Output: 0.5111111
```

---

## The Coefficient of Determination (R²)

- The **coefficient of determination** \( R^2 \) represents the proportion of variability in the dependent variable explained by the regression model.
- It is an indication of **how well the model fits** the data or **predicts future outcomes**.
- In **simple linear regression**, \( R^2 \) is equal to the square of the correlation coefficient \( r \), but this is coincidental for bivariate cases.

### R Output Example

```r
summary(lm(Y ~ X))
```

Sample Output (abridged):

```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)  
(Intercept)  -18.5506    41.4156  -0.448   0.6661
X              1.1855     0.4073   2.911   0.0196 *

Residual standard error: 3.884 on 8 degrees of freedom
Multiple R-squared:  0.5143
Adjusted R-squared:  0.4536
F-statistic: 8.472 on 1 and 8 DF, p-value: 0.01957
```

- **Multiple R-squared**: 0.5143 means ~51% of the variance in Y is explained by X.
- **Adjusted R-squared** adjusts for the number of predictors.
- The **p-value** indicates the statistical significance of the relationship between X and Y.

---

%-----------------------------------------------------------------------%

\subsection{Computing the Correlation Coefficient}

\[ \mbox{Cor(X,Y)} = \frac{\mbox{Cov}(X,Y)}{\sqrt{\mbox{Var}(X) \times \mbox{Var}(Y)}} \]

% http://easycalculation.com/statistics/learn-correlation.php








\subsubsection*{The Adjusted R-square value}
\noindent The adjusted R-square value is found on the summary output for a fitted model. It is called \textbf{\emph{adjusted}} because it takes into account the number of predictor variables being used. The law of parsimony states the simplest model that adequately explains the outcomes is the best. The candidate model with the higher adjusted R squared is considered preferable.

\subsubsection*{The Akaike Information Criterion}
\noindent The AIC is a model selection metric often used in statistics. It is computed using the R command
\texttt{\textbf{AIC()}}. The candidate model with the smallest AIC value is considered preferable.
